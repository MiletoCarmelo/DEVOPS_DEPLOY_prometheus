apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: prometheus-kube-prometheus-kafka-alerts
  namespace: {{ .Release.Namespace }}
  labels:
    app.kubernetes.io/component: monitoring
    app.kubernetes.io/name: kube-prometheus-stack
    app.kubernetes.io/part-of: kube-prometheus-stack
    app.kubernetes.io/version: "v0.68.0"
spec:
  groupVersion: {{ .Values.prometheusRules.groupVersion }}
  groups:
    - name: kubernetes-apps
      rules:
        - alert: PodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[1h]) > 5
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Pod crashing multiple times
            description: 'Pod {{ `{{ $labels.namespace }}` }}/{{ `{{ $labels.pod }}` }} is restarting frequently'
        - alert: PodStuckInPending
          expr: kube_pod_status_phase{phase="Pending"} == 1
          for: 30m
          labels:
            severity: warning
          annotations:
            summary: Pod stuck in Pending state
            description: 'Pod {{ `{{ $labels.namespace }}` }}/{{ `{{ $labels.pod }}` }} has been in pending state for more than 30 minutes'

    - name: memory-alerts
      rules:
        - alert: HighMemoryUsage
          expr: container_memory_usage_bytes / container_spec_memory_limit_bytes * 100 > 85
          for: 15m
          labels:
            severity: warning
          annotations:
            summary: High memory usage detected
            description: 'Container {{ `{{ $labels.container }}` }} in pod {{ `{{ $labels.pod }}` }} is using more than 85% of its memory limit'

    - name: kafka-alerts
      rules:
        - alert: KafkaTopicLag
          expr: kafka_consumergroup_lag > {{ .Values.kafka.lagThreshold | default 5000 }}
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: Kafka consumer group lag
            description: 'Consumer group {{ `{{ $labels.consumergroup }}` }} on topic {{ `{{ $labels.topic }}` }} is lagging by {{ `{{ $value }}` }} messages'